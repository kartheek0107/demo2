{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1IDSogmKL2wwGrEDaVTqa8p1up5hembOo",
      "authorship_tag": "ABX9TyMjQ6Hzv3BT+Tu3y1qOylVu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kartheek0107/demo2/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TC98J9TbEZYn",
        "outputId": "0bcb227e-4093-4adf-cebd-bc747cbe68f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from typing import Optional\n",
        "import math\n",
        "\n",
        "class SEBlock1D(nn.Module):\n",
        "    \"\"\"Squeeze-and-Excitation block for 1D signals\"\"\"\n",
        "    def __init__(self, channels: int, reduction: int = 16):\n",
        "        super().__init__()\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        reduced_channels = max(channels // reduction, 1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv1d(channels, reduced_channels, 1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv1d(reduced_channels, channels, 1, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # x: [B, C, L]\n",
        "        scale = self.global_pool(x)  # [B, C, 1]\n",
        "        scale = self.fc(scale)       # [B, C, 1]\n",
        "        return x * scale\n",
        "\n",
        "class AdaptiveNorm1d(nn.Module):\n",
        "    \"\"\"Adaptive normalization that switches between BatchNorm and GroupNorm based on sequence length\"\"\"\n",
        "    def __init__(self, channels: int, min_length: int = 4):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.min_length = min_length\n",
        "        self.batch_norm = nn.BatchNorm1d(channels)\n",
        "        # Use 8 groups for GroupNorm, ensuring each group has at least 1 channel\n",
        "        num_groups = min(8, channels)\n",
        "        self.group_norm = nn.GroupNorm(num_groups, channels)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # Use GroupNorm if sequence is too short for BatchNorm\n",
        "        if x.size(-1) < self.min_length or self.training is False:\n",
        "            return self.group_norm(x)\n",
        "        else:\n",
        "            return self.batch_norm(x)\n",
        "\n",
        "class ResidualBlock1D(nn.Module):\n",
        "    \"\"\"Residual block with SE attention for 1D signals\"\"\"\n",
        "    def __init__(self, in_channels: int, out_channels: int, stride: int = 1, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3,\n",
        "                              stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = AdaptiveNorm1d(out_channels)  # Use adaptive normalization\n",
        "\n",
        "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3,\n",
        "                              stride=1, padding=1, bias=False)\n",
        "        self.bn2 = AdaptiveNorm1d(out_channels)  # Use adaptive normalization\n",
        "\n",
        "        self.dropout = nn.Dropout1d(dropout) if dropout > 0 else nn.Identity()\n",
        "        self.se = SEBlock1D(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Skip connection\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.skip = nn.Sequential(\n",
        "                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                AdaptiveNorm1d(out_channels)  # Use adaptive normalization\n",
        "            )\n",
        "        else:\n",
        "            self.skip = nn.Identity()\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        identity = self.skip(x)\n",
        "\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.dropout(out)\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out = self.se(out)\n",
        "\n",
        "        out += identity\n",
        "        return self.relu(out)\n",
        "\n",
        "class ASPP1D(nn.Module):\n",
        "    \"\"\"Atrous Spatial Pyramid Pooling for 1D signals\"\"\"\n",
        "    def __init__(self, in_channels: int, out_channels: int = 256):\n",
        "        super().__init__()\n",
        "\n",
        "        # Different dilation rates\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels, out_channels, 1, bias=False),\n",
        "            AdaptiveNorm1d(out_channels),  # Use adaptive normalization\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels, out_channels, 3, padding=2, dilation=2, bias=False),\n",
        "            AdaptiveNorm1d(out_channels),  # Use adaptive normalization\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels, out_channels, 3, padding=4, dilation=4, bias=False),\n",
        "            AdaptiveNorm1d(out_channels),  # Use adaptive normalization\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels, out_channels, 3, padding=8, dilation=8, bias=False),\n",
        "            AdaptiveNorm1d(out_channels),  # Use adaptive normalization\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Global average pooling branch\n",
        "        self.global_pool = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "            nn.Conv1d(in_channels, out_channels, 1, bias=False),\n",
        "            AdaptiveNorm1d(out_channels),  # Use adaptive normalization\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Final projection\n",
        "        self.project = nn.Sequential(\n",
        "            nn.Conv1d(out_channels * 5, out_channels, 1, bias=False),\n",
        "            AdaptiveNorm1d(out_channels),  # Use adaptive normalization\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout1d(0.1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        size = x.size(-1)\n",
        "\n",
        "        # Apply different dilated convolutions\n",
        "        x1 = self.conv1(x)\n",
        "        x2 = self.conv2(x)\n",
        "        x3 = self.conv3(x)\n",
        "        x4 = self.conv4(x)\n",
        "\n",
        "        # Global pooling branch\n",
        "        x5 = self.global_pool(x)\n",
        "        x5 = F.interpolate(x5, size=size, mode='linear', align_corners=False)\n",
        "\n",
        "        # Concatenate all branches\n",
        "        out = torch.cat([x1, x2, x3, x4, x5], dim=1)\n",
        "        return self.project(out)\n",
        "\n",
        "class AttentionGate1D(nn.Module):\n",
        "    \"\"\"Attention gate for skip connections\"\"\"\n",
        "    def __init__(self, gate_channels: int, skip_channels: int, inter_channels: Optional[int] = None):\n",
        "        super().__init__()\n",
        "\n",
        "        if inter_channels is None:\n",
        "            inter_channels = skip_channels // 2\n",
        "\n",
        "        self.gate_conv = nn.Conv1d(gate_channels, inter_channels, 1, bias=False)\n",
        "        self.skip_conv = nn.Conv1d(skip_channels, inter_channels, 1, bias=False)\n",
        "        self.psi_conv = nn.Conv1d(inter_channels, 1, 1, bias=True)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, gate: Tensor, skip: Tensor) -> Tensor:\n",
        "        # gate: gating signal from decoder, skip: skip connection from encoder\n",
        "        g = self.gate_conv(gate)\n",
        "        s = self.skip_conv(skip)\n",
        "\n",
        "        # Upsample gate signal if needed\n",
        "        if g.size(-1) != s.size(-1):\n",
        "            g = F.interpolate(g, size=s.size(-1), mode='linear', align_corners=False)\n",
        "\n",
        "        psi = self.relu(g + s)\n",
        "        psi = torch.sigmoid(self.psi_conv(psi))\n",
        "\n",
        "        return skip * psi\n",
        "\n",
        "class DecoderBlock1D(nn.Module):\n",
        "    \"\"\"Decoder block with attention-gated skip connections\"\"\"\n",
        "    def __init__(self, in_channels: int, skip_channels: int, out_channels: int,\n",
        "                 use_attention: bool = True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.upsample = nn.ConvTranspose1d(in_channels, out_channels,\n",
        "                                         kernel_size=2, stride=2, bias=False)\n",
        "\n",
        "        self.use_attention = use_attention\n",
        "        if use_attention:\n",
        "            self.attention = AttentionGate1D(out_channels, skip_channels)\n",
        "\n",
        "        # Process concatenated features\n",
        "        concat_channels = out_channels + skip_channels\n",
        "        self.conv_block = nn.Sequential(\n",
        "            ResidualBlock1D(concat_channels, out_channels),\n",
        "            ResidualBlock1D(out_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor, skip: Tensor) -> Tensor:\n",
        "        # Upsample decoder features\n",
        "        x = self.upsample(x)\n",
        "\n",
        "        # Handle size mismatch\n",
        "        if x.size(-1) != skip.size(-1):\n",
        "            diff = skip.size(-1) - x.size(-1)\n",
        "            x = F.pad(x, (diff // 2, diff - diff // 2))\n",
        "\n",
        "        # Apply attention to skip connection\n",
        "        if self.use_attention:\n",
        "            skip = self.attention(x, skip)\n",
        "\n",
        "        # Concatenate and process\n",
        "        x = torch.cat([x, skip], dim=1)\n",
        "        return self.conv_block(x)\n",
        "\n",
        "class ResUNetPP1D(nn.Module):\n",
        "    \"\"\"1D ResUNet++ with reconstruction head for signal reconstruction\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_channels: int = 20,\n",
        "                 out_channels: int = 20,\n",
        "                 base_filters: int = 64,\n",
        "                 depth: int = 3,  # Reduced depth to prevent sequence length becoming 1\n",
        "                 dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.depth = depth\n",
        "        filters = [base_filters * (2 ** i) for i in range(depth + 1)]\n",
        "\n",
        "        # Initial convolution\n",
        "        self.input_conv = nn.Sequential(\n",
        "            nn.Conv1d(in_channels, filters[0], kernel_size=7, padding=3, bias=False),\n",
        "            AdaptiveNorm1d(filters[0]),  # Use adaptive normalization\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Encoder\n",
        "        self.encoders = nn.ModuleList()\n",
        "        self.pools = nn.ModuleList()\n",
        "\n",
        "        for i in range(depth):\n",
        "            # Encoder block\n",
        "            if i == 0:\n",
        "                encoder = nn.Sequential(\n",
        "                    ResidualBlock1D(filters[i], filters[i], dropout=dropout),\n",
        "                    ResidualBlock1D(filters[i], filters[i], dropout=dropout)\n",
        "                )\n",
        "            else:\n",
        "                encoder = nn.Sequential(\n",
        "                    ResidualBlock1D(filters[i-1], filters[i], dropout=dropout),\n",
        "                    ResidualBlock1D(filters[i], filters[i], dropout=dropout)\n",
        "                )\n",
        "            self.encoders.append(encoder)\n",
        "\n",
        "            # Pooling\n",
        "            self.pools.append(nn.MaxPool1d(kernel_size=2, stride=2))\n",
        "\n",
        "        # Bottleneck with ASPP\n",
        "        bottleneck_filters = filters[depth]\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            ResidualBlock1D(filters[depth-1], bottleneck_filters, dropout=dropout),\n",
        "            ASPP1D(bottleneck_filters, bottleneck_filters // 2),\n",
        "            ResidualBlock1D(bottleneck_filters // 2, bottleneck_filters, dropout=dropout)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoders = nn.ModuleList()\n",
        "        for i in range(depth):\n",
        "            decoder_in = filters[depth - i]\n",
        "            skip_channels = filters[depth - i - 1]\n",
        "            decoder_out = filters[depth - i - 1]\n",
        "\n",
        "            self.decoders.append(\n",
        "                DecoderBlock1D(decoder_in, skip_channels, decoder_out, use_attention=True)\n",
        "            )\n",
        "\n",
        "        # Reconstruction head\n",
        "        self.reconstruction_head = nn.Sequential(\n",
        "            nn.Conv1d(filters[0], filters[0] // 2, kernel_size=3, padding=1),\n",
        "            AdaptiveNorm1d(filters[0] // 2),  # Use adaptive normalization\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout1d(dropout),\n",
        "            nn.Conv1d(filters[0] // 2, out_channels, kernel_size=1)\n",
        "        )\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"Initialize network weights\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv1d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, (nn.BatchNorm1d, nn.GroupNorm)):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.ConvTranspose1d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # Store original length for final interpolation\n",
        "        orig_length = x.size(-1)\n",
        "\n",
        "        # Initial convolution\n",
        "        x = self.input_conv(x)\n",
        "\n",
        "        # Encoder path - store skip connections\n",
        "        skips = []\n",
        "        for i in range(self.depth):\n",
        "            x = self.encoders[i](x)\n",
        "            skips.append(x)\n",
        "            x = self.pools[i](x)\n",
        "\n",
        "        # Bottleneck\n",
        "        x = self.bottleneck(x)\n",
        "\n",
        "        # Decoder path\n",
        "        for i in range(self.depth):\n",
        "            skip = skips[self.depth - i - 1]\n",
        "            x = self.decoders[i](x, skip)\n",
        "\n",
        "        # Reconstruction head\n",
        "        x = self.reconstruction_head(x)\n",
        "\n",
        "        # Ensure output matches input length\n",
        "        if x.size(-1) != orig_length:\n",
        "            x = F.interpolate(x, size=orig_length, mode='linear', align_corners=False)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Sliding window processing for long sequences\n",
        "class SlidingWindowProcessor:\n",
        "    \"\"\"Process long sequences using sliding window with overlap-add\"\"\"\n",
        "\n",
        "    def __init__(self, model: ResUNetPP1D, window_size: int = 1024, stride: int = 128):\n",
        "        self.model = model\n",
        "        self.window_size = window_size\n",
        "        self.stride = stride\n",
        "        self.overlap = window_size - stride\n",
        "\n",
        "        # Create blending weights for overlap regions\n",
        "        self.blend_weights = self._create_blend_weights()\n",
        "\n",
        "    def _create_blend_weights(self) -> Tensor:\n",
        "        \"\"\"Create weights for blending overlapping windows\"\"\"\n",
        "        weights = torch.ones(self.window_size)\n",
        "        if self.overlap > 0:\n",
        "            # Linear fade in/out for overlap regions\n",
        "            fade_length = self.overlap\n",
        "            weights[:fade_length] = torch.linspace(0, 1, fade_length)\n",
        "            weights[-fade_length:] = torch.linspace(1, 0, fade_length)\n",
        "        return weights.view(1, 1, -1)\n",
        "\n",
        "    def process(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"Process input using sliding window\"\"\"\n",
        "        B, C, L = x.shape\n",
        "        device = x.device\n",
        "\n",
        "        if L <= self.window_size:\n",
        "            return self.model(x)\n",
        "\n",
        "        # Pad input if necessary\n",
        "        pad_length = 0\n",
        "        if (L - self.window_size) % self.stride != 0:\n",
        "            pad_length = self.stride - ((L - self.window_size) % self.stride)\n",
        "            x = F.pad(x, (0, pad_length), mode='reflect')\n",
        "            L += pad_length\n",
        "\n",
        "        # Initialize output\n",
        "        output = torch.zeros_like(x)\n",
        "        weights_sum = torch.zeros_like(x)\n",
        "        blend_weights = self.blend_weights.to(device)\n",
        "\n",
        "        # Process windows\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for start in range(0, L - self.window_size + 1, self.stride):\n",
        "                end = start + self.window_size\n",
        "                window = x[:, :, start:end]\n",
        "\n",
        "                # Process window\n",
        "                window_output = self.model(window)\n",
        "\n",
        "                # Apply blending weights\n",
        "                weighted_output = window_output * blend_weights\n",
        "\n",
        "                # Accumulate\n",
        "                output[:, :, start:end] += weighted_output\n",
        "                weights_sum[:, :, start:end] += blend_weights\n",
        "\n",
        "        # Normalize by accumulated weights\n",
        "        output = output / (weights_sum + 1e-8)\n",
        "\n",
        "        # Remove padding\n",
        "        if pad_length > 0:\n",
        "            output = output[:, :, :-pad_length]\n",
        "\n",
        "        return output\n",
        "\n",
        "# Training utilities\n",
        "def reconstruction_loss(pred: Tensor, target: Tensor) -> Tensor:\n",
        "    \"\"\"MSE loss for reconstruction\"\"\"\n",
        "    return F.mse_loss(pred, target)\n",
        "\n",
        "def create_model_and_processor(in_channels: int = 20,\n",
        "                             out_channels: int = 20,\n",
        "                             window_size: int = 1024,\n",
        "                             stride: int = 128) -> tuple:\n",
        "    \"\"\"Create model and sliding window processor\"\"\"\n",
        "    model = ResUNetPP1D(in_channels=in_channels, out_channels=out_channels)\n",
        "    processor = SlidingWindowProcessor(model, window_size=window_size, stride=stride)\n",
        "    return model, processor\n",
        "\n",
        "# Example usage and testing\n",
        "if __name__ == \"__main__\":\n",
        "    # Test the model\n",
        "    print(\"Creating ResUNet++ 1D model...\")\n",
        "    model, processor = create_model_and_processor(in_channels=20, out_channels=20)\n",
        "\n",
        "    # Print model info\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "    # Test with window size input\n",
        "    print(f\"\\nTesting with window size input (1024)...\")\n",
        "    test_input = torch.randn(1, 20, 1024)\n",
        "    with torch.no_grad():\n",
        "        output = model(test_input)\n",
        "    print(f\"Input shape: {test_input.shape}\")\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "\n",
        "    # Test with long sequence using sliding window\n",
        "    print(f\"\\nTesting with long sequence (5000) using sliding window...\")\n",
        "    long_input = torch.randn(1, 20, 5000)\n",
        "    with torch.no_grad():\n",
        "        long_output = processor.process(long_input)\n",
        "    print(f\"Long input shape: {long_input.shape}\")\n",
        "    print(f\"Long output shape: {long_output.shape}\")\n",
        "\n",
        "    # Test loss computation\n",
        "    target = torch.randn_like(output)\n",
        "    loss = reconstruction_loss(output, target)\n",
        "    print(f\"Reconstruction loss: {loss.item():.6f}\")\n",
        "\n",
        "    print(\"\\nModel created and tested successfully!\")\n",
        "    print(\"Ready for training with MSE reconstruction loss.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2E6-DtR57T2P",
        "outputId": "ed653c45-8584-461c-fc88-143922386f10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating ResUNet++ 1D model...\n",
            "Total parameters: 7,394,583\n",
            "Trainable parameters: 7,394,583\n",
            "\n",
            "Testing with window size input (1024)...\n",
            "Input shape: torch.Size([1, 20, 1024])\n",
            "Output shape: torch.Size([1, 20, 1024])\n",
            "\n",
            "Testing with long sequence (5000) using sliding window...\n",
            "Long input shape: torch.Size([1, 20, 5000])\n",
            "Long output shape: torch.Size([1, 20, 5000])\n",
            "Reconstruction loss: 2.282301\n",
            "\n",
            "Model created and tested successfully!\n",
            "Ready for training with MSE reconstruction loss.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quiet Window Reconstruction Training for ResUNet++ 1D\n",
        "# Specifically designed for your multi-class labeled space weather data\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "class QuietWindowDataset(Dataset):\n",
        "    \"\"\"Dataset specifically for quiet window reconstruction\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_dir: str,\n",
        "                 target_class: int = 0,  # 0 = quiet, 1 = sep, 2 = cme\n",
        "                 add_noise: bool = True,\n",
        "                 noise_std: float = 0.1,\n",
        "                 normalize: bool = True,\n",
        "                 validation_split: bool = False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_dir: Directory containing .npz files\n",
        "            target_class: Class to use for reconstruction (0=quiet, 1=sep, 2=cme)\n",
        "            add_noise: Whether to add noise to input (for denoising task)\n",
        "            noise_std: Standard deviation of noise to add\n",
        "            normalize: Whether to normalize features\n",
        "            validation_split: Whether this is validation data\n",
        "        \"\"\"\n",
        "        self.data_dir = data_dir\n",
        "        self.target_class = target_class\n",
        "        self.add_noise = add_noise\n",
        "        self.noise_std = noise_std\n",
        "        self.normalize = normalize\n",
        "        self.class_names = [\"quiet\", \"sep\", \"cme\"]\n",
        "\n",
        "        print(f\"üéØ Loading {self.class_names[target_class]} windows for reconstruction...\")\n",
        "\n",
        "        # Load all .npz files\n",
        "        self.npz_files = glob.glob(os.path.join(data_dir, \"*.npz\"))\n",
        "        if not self.npz_files:\n",
        "            raise ValueError(f\"No .npz files found in {data_dir}\")\n",
        "\n",
        "        print(f\"üìÅ Found {len(self.npz_files)} .npz files\")\n",
        "\n",
        "        # Load and filter data\n",
        "        self.windows, self.labels, self.feature_names = self._load_and_filter_data()\n",
        "\n",
        "        # Normalize if requested\n",
        "        if self.normalize:\n",
        "            self._normalize_data()\n",
        "\n",
        "        print(f\"‚úÖ Dataset ready:\")\n",
        "        print(f\"  - {len(self.windows)} {self.class_names[target_class]} windows\")\n",
        "        print(f\"  - Shape: {self.windows.shape}\")\n",
        "        print(f\"  - Features: {len(self.feature_names)}\")\n",
        "        print(f\"  - Add noise: {add_noise} (std={noise_std})\")\n",
        "\n",
        "    def _load_and_filter_data(self):\n",
        "        \"\"\"Load data and filter for target class\"\"\"\n",
        "        all_windows = []\n",
        "        all_labels = []\n",
        "        feature_names = None\n",
        "\n",
        "        for file_path in tqdm(self.npz_files, desc=\"Loading files\"):\n",
        "            try:\n",
        "                data = np.load(file_path, allow_pickle=True)\n",
        "\n",
        "                # Extract data\n",
        "                windows = data[\"windows\"]  # (N, 1024, 20)\n",
        "                labels = data[\"labels\"]    # (N,)\n",
        "\n",
        "                if feature_names is None:\n",
        "                    feature_names = data[\"feature_names\"]\n",
        "\n",
        "                # Filter for target class\n",
        "                target_mask = labels == self.target_class\n",
        "                target_windows = windows[target_mask]\n",
        "                target_labels = labels[target_mask]\n",
        "\n",
        "                if len(target_windows) > 0:\n",
        "                    all_windows.append(target_windows)\n",
        "                    all_labels.append(target_labels)\n",
        "\n",
        "                print(f\"  {os.path.basename(file_path)}: {len(target_windows)}/{len(windows)} {self.class_names[self.target_class]} windows\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error loading {file_path}: {e}\")\n",
        "                continue\n",
        "\n",
        "        if not all_windows:\n",
        "            raise ValueError(f\"No {self.class_names[self.target_class]} windows found!\")\n",
        "\n",
        "        # Combine all data\n",
        "        combined_windows = np.concatenate(all_windows, axis=0).astype(np.float32)\n",
        "        combined_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "        # Transpose to (N, channels, time) for PyTorch\n",
        "        combined_windows = combined_windows.transpose(0, 2, 1)  # (N, 20, 1024)\n",
        "\n",
        "        return combined_windows, combined_labels, feature_names\n",
        "\n",
        "    def _normalize_data(self):\n",
        "        \"\"\"Normalize data across all samples and time points\"\"\"\n",
        "        # Calculate statistics per channel\n",
        "        self.data_mean = np.mean(self.windows, axis=(0, 2), keepdims=True)  # (1, 20, 1)\n",
        "        self.data_std = np.std(self.windows, axis=(0, 2), keepdims=True) + 1e-8\n",
        "\n",
        "        # Normalize\n",
        "        self.windows = (self.windows - self.data_mean) / self.data_std\n",
        "\n",
        "        print(f\"üìä Data normalized:\")\n",
        "        print(f\"  Mean range: [{self.data_mean.min():.4f}, {self.data_mean.max():.4f}]\")\n",
        "        print(f\"  Std range: [{self.data_std.min():.4f}, {self.data_std.max():.4f}]\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.windows)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get clean target\n",
        "        target = torch.tensor(self.windows[idx])  # (20, 1024)\n",
        "\n",
        "        # Create input (potentially noisy)\n",
        "        if self.add_noise:\n",
        "            noise = torch.randn_like(target) * self.noise_std\n",
        "            input_data = target + noise\n",
        "        else:\n",
        "            input_data = target.clone()\n",
        "\n",
        "        return input_data, target\n",
        "\n",
        "class ReconstructionTrainer:\n",
        "    \"\"\"Trainer specifically for quiet window reconstruction\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 model: nn.Module,\n",
        "                 train_loader: DataLoader,\n",
        "                 val_loader: DataLoader,\n",
        "                 device: str = 'cuda',\n",
        "                 learning_rate: float = 1e-3,\n",
        "                 save_dir: str = '/content/drive/MyDrive/quiet_reconstruction'):\n",
        "\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.save_dir = save_dir\n",
        "\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        # Optimizer with different learning rates for different parts\n",
        "        encoder_params = []\n",
        "        decoder_params = []\n",
        "        head_params = []\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "            if 'reconstruction_head' in name:\n",
        "                head_params.append(param)\n",
        "            elif any(x in name for x in ['encoder', 'bottleneck']):\n",
        "                encoder_params.append(param)\n",
        "            else:\n",
        "                decoder_params.append(param)\n",
        "\n",
        "        # Higher learning rate for reconstruction head\n",
        "        self.optimizer = optim.AdamW([\n",
        "            {'params': encoder_params, 'lr': learning_rate * 0.1},  # Lower LR for encoder\n",
        "            {'params': decoder_params, 'lr': learning_rate * 0.5},  # Medium LR for decoder\n",
        "            {'params': head_params, 'lr': learning_rate}            # Full LR for head\n",
        "        ], weight_decay=1e-5)\n",
        "\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer, mode='min', factor=0.7, patience=15, verbose=True\n",
        "        )\n",
        "\n",
        "        # Loss functions\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "        self.l1_loss = nn.L1Loss()\n",
        "\n",
        "        # Tracking\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.best_val_loss = float('inf')\n",
        "\n",
        "        self.writer = SummaryWriter(os.path.join(save_dir, 'logs'))\n",
        "\n",
        "        print(f\"üèãÔ∏è Trainer initialized:\")\n",
        "        print(f\"  Device: {device}\")\n",
        "        print(f\"  Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "        print(f\"  Save directory: {save_dir}\")\n",
        "\n",
        "    def reconstruction_loss(self, pred, target):\n",
        "        \"\"\"Combined reconstruction loss\"\"\"\n",
        "        mse = self.mse_loss(pred, target)\n",
        "        l1 = self.l1_loss(pred, target)\n",
        "        return 0.7 * mse + 0.3 * l1  # Weighted combination\n",
        "\n",
        "    def train_epoch(self):\n",
        "        \"\"\"Train for one epoch\"\"\"\n",
        "        self.model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        pbar = tqdm(self.train_loader, desc='Training', leave=False)\n",
        "        for batch_idx, (data, target) in enumerate(pbar):\n",
        "            data, target = data.to(self.device), target.to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            output = self.model(data)\n",
        "            loss = self.reconstruction_loss(output, target)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            pbar.set_postfix({'Loss': f'{loss.item():.6f}'})\n",
        "\n",
        "        return total_loss / len(self.train_loader)\n",
        "\n",
        "    def validate(self):\n",
        "        \"\"\"Validate the model\"\"\"\n",
        "        self.model.eval()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, target in tqdm(self.val_loader, desc='Validation', leave=False):\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "                output = self.model(data)\n",
        "                loss = self.reconstruction_loss(output, target)\n",
        "                total_loss += loss.item()\n",
        "\n",
        "        return total_loss / len(self.val_loader)\n",
        "\n",
        "    def visualize_reconstruction(self, num_samples=3):\n",
        "        \"\"\"Visualize reconstruction results\"\"\"\n",
        "        self.model.eval()\n",
        "        fig, axes = plt.subplots(num_samples, 1, figsize=(15, 4*num_samples))\n",
        "        if num_samples == 1:\n",
        "            axes = [axes]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            data_iter = iter(self.val_loader)\n",
        "            data, target = next(data_iter)\n",
        "            data, target = data.to(self.device), target.to(self.device)\n",
        "\n",
        "            output = self.model(data)\n",
        "\n",
        "            for i in range(min(num_samples, data.size(0))):\n",
        "                # Plot first few channels\n",
        "                channels_to_plot = min(5, data.size(1))\n",
        "                for ch in range(channels_to_plot):\n",
        "                    axes[i].plot(target[i, ch].cpu().numpy(),\n",
        "                               label=f'Target Ch{ch}', alpha=0.7, linewidth=1)\n",
        "                    axes[i].plot(output[i, ch].cpu().numpy(),\n",
        "                               label=f'Reconstructed Ch{ch}', alpha=0.7, linewidth=1, linestyle='--')\n",
        "\n",
        "                axes[i].set_title(f'Sample {i+1} - Reconstruction vs Target')\n",
        "                axes[i].legend()\n",
        "                axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.save_dir, 'reconstruction_samples.png'), dpi=150)\n",
        "        plt.show()\n",
        "\n",
        "    def save_checkpoint(self, epoch, is_best=False):\n",
        "        \"\"\"Save checkpoint\"\"\"\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'train_losses': self.train_losses,\n",
        "            'val_losses': self.val_losses,\n",
        "            'best_val_loss': self.best_val_loss\n",
        "        }\n",
        "\n",
        "        torch.save(checkpoint, os.path.join(self.save_dir, 'latest_checkpoint.pt'))\n",
        "\n",
        "        if is_best:\n",
        "            torch.save(checkpoint, os.path.join(self.save_dir, 'best_checkpoint.pt'))\n",
        "            print(f\"üèÜ New best model! Val Loss: {self.best_val_loss:.6f}\")\n",
        "\n",
        "    def train(self, num_epochs):\n",
        "        \"\"\"Main training loop\"\"\"\n",
        "        print(f\"üöÄ Training quiet window reconstruction for {num_epochs} epochs...\")\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f\"\\nüìä Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "            # Train and validate\n",
        "            train_loss = self.train_epoch()\n",
        "            val_loss = self.validate()\n",
        "\n",
        "            # Update tracking\n",
        "            self.train_losses.append(train_loss)\n",
        "            self.val_losses.append(val_loss)\n",
        "            self.scheduler.step(val_loss)\n",
        "\n",
        "            # Logging\n",
        "            self.writer.add_scalar('Loss/Train', train_loss, epoch)\n",
        "            self.writer.add_scalar('Loss/Val', val_loss, epoch)\n",
        "\n",
        "            print(f\"Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
        "\n",
        "            # Save checkpoint\n",
        "            is_best = val_loss < self.best_val_loss\n",
        "            if is_best:\n",
        "                self.best_val_loss = val_loss\n",
        "            self.save_checkpoint(epoch + 1, is_best)\n",
        "\n",
        "            # Visualize every 20 epochs\n",
        "            if (epoch + 1) % 20 == 0:\n",
        "                self.visualize_reconstruction()\n",
        "                self.plot_losses()\n",
        "\n",
        "        print(f\"üéâ Training completed! Best Val Loss: {self.best_val_loss:.6f}\")\n",
        "        self.writer.close()\n",
        "\n",
        "    def plot_losses(self):\n",
        "        \"\"\"Plot training progress\"\"\"\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(self.train_losses, label='Training Loss', alpha=0.8)\n",
        "        plt.plot(self.val_losses, label='Validation Loss', alpha=0.8)\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Reconstruction Training Progress')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        recent_epochs = min(20, len(self.val_losses))\n",
        "        plt.plot(self.val_losses[-recent_epochs:], 'o-', color='orange')\n",
        "        plt.xlabel('Recent Epochs')\n",
        "        plt.ylabel('Validation Loss')\n",
        "        plt.title('Recent Validation Loss')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.save_dir, 'training_progress.png'), dpi=150)\n",
        "        plt.show()\n",
        "\n",
        "def setup_quiet_reconstruction():\n",
        "    \"\"\"Setup and start training for quiet window reconstruction\"\"\"\n",
        "\n",
        "    # Configuration\n",
        "    config = {\n",
        "        'data_dir': '/content/drive/MyDrive/labelled_windows_multiclass',\n",
        "        'target_class': 0,        # 0=quiet, 1=sep, 2=cme\n",
        "        'batch_size': 4,         # Adjust based on GPU memory\n",
        "        'learning_rate': 1e-3,\n",
        "        'num_epochs': 80,\n",
        "        'val_split': 0.2,\n",
        "        'add_noise': True,        # Add noise for denoising task\n",
        "        'noise_std': 0.1,         # Noise standard deviation\n",
        "        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "        'save_dir': '/content/drive/MyDrive/quiet_reconstruction_model',\n",
        "    }\n",
        "\n",
        "    print(\"üéØ QUIET WINDOW RECONSTRUCTION TRAINING\")\n",
        "    print(\"=\" * 50)\n",
        "    for key, value in config.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Create dataset\n",
        "    print(\"\\nüìä Creating dataset...\")\n",
        "    full_dataset = QuietWindowDataset(\n",
        "        data_dir=config['data_dir'],\n",
        "        target_class=config['target_class'],\n",
        "        add_noise=config['add_noise'],\n",
        "        noise_std=config['noise_std'],\n",
        "        normalize=True\n",
        "    )\n",
        "\n",
        "    # Get sample to determine dimensions\n",
        "    sample_input, sample_target = full_dataset[0]\n",
        "    n_channels = sample_input.shape[0]  # Should be 20\n",
        "    seq_length = sample_input.shape[1]   # Should be 1024\n",
        "\n",
        "    print(f\"üìè Data dimensions: {n_channels} channels, {seq_length} time points\")\n",
        "\n",
        "    # Split dataset\n",
        "    val_size = int(len(full_dataset) * config['val_split'])\n",
        "    train_size = len(full_dataset) - val_size\n",
        "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'],\n",
        "                            shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'],\n",
        "                          shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    print(f\"‚úÖ Data loaders created: {len(train_dataset)} train, {len(val_dataset)} val\")\n",
        "\n",
        "    # Create model\n",
        "    print(f\"\\nüß† Creating ResUNet++ model...\")\n",
        "    model = ResUNetPP1D(\n",
        "        in_channels=n_channels,\n",
        "        out_channels=n_channels,\n",
        "        base_filters=64,\n",
        "        depth=3,\n",
        "        dropout=0.15\n",
        "    )\n",
        "\n",
        "    # Create trainer\n",
        "    trainer = ReconstructionTrainer(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        device=config['device'],\n",
        "        learning_rate=config['learning_rate'],\n",
        "        save_dir=config['save_dir']\n",
        "    )\n",
        "\n",
        "    # Save config\n",
        "    config['n_channels'] = n_channels\n",
        "    config['seq_length'] = seq_length\n",
        "    with open(os.path.join(config['save_dir'], 'config.json'), 'w') as f:\n",
        "        json.dump(config, f, indent=2)\n",
        "\n",
        "    # Start training\n",
        "    trainer.train(config['num_epochs'])\n",
        "\n",
        "    return trainer, model, full_dataset\n",
        "\n",
        "# üöÄ READY TO RUN!\n",
        "print(\"üéØ Quiet Window Reconstruction Training Setup Ready!\")\n",
        "print(\"\\nThis will:\")\n",
        "print(\"  ‚úÖ Load only QUIET windows from your data\")\n",
        "print(\"  ‚úÖ Add noise for denoising reconstruction task\")\n",
        "print(\"  ‚úÖ Train ResUNet++ to reconstruct clean quiet windows\")\n",
        "print(\"  ‚úÖ Focus training on the reconstruction head\")\n",
        "print(\"  ‚úÖ Save best model for quiet window reconstruction\")\n",
        "print(\"\\nüöÄ Run: trainer, model, dataset = setup_quiet_reconstruction()\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIt0HarV9sP2",
        "outputId": "c14914e6-fec4-47a6-b2f0-6fc8bcff21b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "üéØ Quiet Window Reconstruction Training Setup Ready!\n",
            "\n",
            "This will:\n",
            "  ‚úÖ Load only QUIET windows from your data\n",
            "  ‚úÖ Add noise for denoising reconstruction task\n",
            "  ‚úÖ Train ResUNet++ to reconstruct clean quiet windows\n",
            "  ‚úÖ Focus training on the reconstruction head\n",
            "  ‚úÖ Save best model for quiet window reconstruction\n",
            "\n",
            "üöÄ Run: trainer, model, dataset = setup_quiet_reconstruction()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training with your data\n",
        "trainer, model = setup_quiet_reconstruction()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "id": "g_kGLGScAyz6",
        "outputId": "c1e155aa-cb12-40e7-df37-63a1f0e298d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ QUIET WINDOW RECONSTRUCTION TRAINING\n",
            "==================================================\n",
            "  data_dir: /content/drive/MyDrive/labelled_windows_multiclass\n",
            "  target_class: 0\n",
            "  batch_size: 4\n",
            "  learning_rate: 0.001\n",
            "  num_epochs: 80\n",
            "  val_split: 0.2\n",
            "  add_noise: True\n",
            "  noise_std: 0.1\n",
            "  device: cuda\n",
            "  save_dir: /content/drive/MyDrive/quiet_reconstruction_model\n",
            "==================================================\n",
            "\n",
            "üìä Creating dataset...\n",
            "üéØ Loading quiet windows for reconstruction...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "No .npz files found in /content/drive/MyDrive/labelled_windows_multiclass",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2435972051.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Start training with your data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup_quiet_reconstruction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3299454226.py\u001b[0m in \u001b[0;36msetup_quiet_reconstruction\u001b[0;34m()\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;31m# Create dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nüìä Creating dataset...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m     full_dataset = QuietWindowDataset(\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data_dir'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0mtarget_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target_class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3299454226.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_dir, target_class, add_noise, noise_std, normalize, validation_split)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnpz_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"*.npz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnpz_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No .npz files found in {data_dir}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"üìÅ Found {len(self.npz_files)} .npz files\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No .npz files found in /content/drive/MyDrive/labelled_windows_multiclass"
          ]
        }
      ]
    }
  ]
}